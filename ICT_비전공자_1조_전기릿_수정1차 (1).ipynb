{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 23224,
     "status": "ok",
     "timestamp": 1756297483106,
     "user": {
      "displayName": "ê¹€ì¤‘ì„œ",
      "userId": "17755098182793568770"
     },
     "user_tz": -540
    },
    "id": "nmyGJ9swX6jo",
    "outputId": "8c01c5dd-1af6-46d2-993c-3500f4debfa8"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai tiktoken python-dotenv gradio sympy schemdraw pypdf faiss-cpu langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "executionInfo": {
     "elapsed": 306977,
     "status": "ok",
     "timestamp": 1756298543076,
     "user": {
      "displayName": "ê¹€ì¤‘ì„œ",
      "userId": "17755098182793568770"
     },
     "user_tz": -540
    },
    "id": "MUzcr11NX1ba",
    "outputId": "ff10e93c-563d-4c6e-be86-6264dd23b233"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_16864/4046869562.py:167: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = gr.State(lambda: ConversationBufferMemory(memory_key=\"history\", return_messages=True))\n",
      "/tmp/ipykernel_16864/4046869562.py:189: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_display = gr.Chatbot(height=520, show_copy_button=True, label=\"ëŒ€í™”ì°½\")\n",
      "/tmp/ipykernel_16864/4046869562.py:253: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  doc_chatbot = gr.Chatbot(label=\"ë¬¸ì„œ Q&A\", height=450)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Missing file: /home/codespace/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64\n",
      "2. Rename the downloaded file to: frpc_linux_amd64_v0.3\n",
      "3. Move the file to this location: /home/codespace/.cache/huggingface/gradio/frpc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# [ìµœì¢… í†µí•©ë³¸] ì „ê¸°Â·ì „ì ì¢…í•© ì–´ì‹œìŠ¤í„´íŠ¸\n",
    "\n",
    "# ----------------------------->\n",
    "# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# ----------------------------->\n",
    "import os\n",
    "import json\n",
    "import cmath\n",
    "import math\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# Gradio ë° LLM ê´€ë ¨\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import gradio as gr\n",
    "\n",
    "# RAG (ë¬¸ì„œ Q&A) ê´€ë ¨\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# ê³„ì‚° ë° íšŒë¡œë„ ê´€ë ¨\n",
    "try:\n",
    "    import sympy as sp\n",
    "    import schemdraw\n",
    "    import schemdraw.elements as elm\n",
    "except ImportError:\n",
    "    print(\"sympy, schemdraw ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. !pip install sympy schemdraw ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    sp, schemdraw, elm = None, None, None\n",
    "\n",
    "# ----------------------------->\n",
    "# 1. í™˜ê²½ ì„¤ì • ë° LLM ì¸ìŠ¤í„´ìŠ¤\n",
    "# ----------------------------->\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    # .env íŒŒì¼ì— í‚¤ê°€ ì—†ë‹¤ë©´ ì—¬ê¸°ì„œ ì§ì ‘ ì„¤ì •í•´ì£¼ì„¸ìš”.\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise RuntimeError(\"OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# LLMì„ ì—­í• ì— ë”°ë¼ ë¶„ë¦¬\n",
    "llm_guard = ChatOpenAI(model=OPENAI_MODEL, temperature=0) # íŒì •, ë¶„ë¥˜ ë“± ì—„ê²©í•œ ì—­í• \n",
    "llm_gen = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)  # ìš”ì•½, ìƒì„±, ëŒ€í™” ë“± ì°½ì˜ì ì¸ ì—­í• \n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# ----------------------------->\n",
    "# 2. ê¸°ì¡´ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ (ìš”ì•½ ì±—ë´‡, ê³„ì‚°ê¸° ë“±)\n",
    "# ----------------------------->\n",
    "KNOWLEDGE_CUTOFF = \"2024-06\"\n",
    "STYLE_SYS = (\n",
    "    \"í•œêµ­ì–´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. \"\n",
    "    \"í•µì‹¬ì€ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ë˜, ì „ë ¥Â·ì—ë„ˆì§€Â·ëª¨ë¹Œë¦¬í‹° ë¶„ì•¼ì˜ ìˆ˜ì¹˜/ë‹¨ìœ„/ê¸°í˜¸(Î·, THD, pf, pu, kW, kWh, Â°C ë“±)ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤. \"\n",
    "    \"ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ê¸°ì–µì´ ëª¨í˜¸í•œ ë‚´ìš©ì€ 'ë¶ˆí™•ì‹¤'ë¡œ í‘œì‹œí•˜ê³  ì¶”ì •Â·ì¼ë°˜ë¡ ì€ ëª…í™•íˆ êµ¬ë¶„í•©ë‹ˆë‹¤. ê³¼ì¥ í‘œí˜„ì€ ì§€ì–‘í•©ë‹ˆë‹¤.\"\n",
    ")\n",
    "def calculate_series_resistance(resistances: List[float]) -> float:\n",
    "    \"\"\"ì§ë ¬ ì—°ê²°ëœ ì €í•­ë“¤ì˜ ì´ ì €í•­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return sum(resistances) if resistances else 0.0\n",
    "\n",
    "def calculate_parallel_resistance(resistances: List[float]) -> float:\n",
    "    \"\"\"ë³‘ë ¬ ì—°ê²°ëœ ì €í•­ë“¤ì˜ ì´ ì €í•­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not resistances: return 0.0\n",
    "    if 0.0 in resistances: return 0.0\n",
    "    sum_of_reciprocals = sum(1.0 / r for r in resistances)\n",
    "    return 1.0 / sum_of_reciprocals if sum_of_reciprocals != 0 else float('inf')\n",
    "\n",
    "def resistor_color_code(res_val: float) -> str:\n",
    "    \"\"\"ì €í•­ê°’(Î©)ì„ ì…ë ¥ë°›ì•„ 4-band ìƒ‰ê¹” ë ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if res_val < 0:\n",
    "        return \"ì˜¤ë¥˜: ì €í•­ê°’ì€ ìŒìˆ˜ì¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    if res_val == 0:\n",
    "        return \"ê²€ì •(0) - ê²€ì •(0) - ê²€ì •(x1)\"\n",
    "\n",
    "    colors = {\n",
    "        0: \"ê²€ì •\", 1: \"ê°ˆìƒ‰\", 2: \"ë¹¨ê°•\", 3: \"ì£¼í™©\", 4: \"ë…¸ë‘\",\n",
    "        5: \"ì´ˆë¡\", 6: \"íŒŒë‘\", 7: \"ë³´ë¼\", 8: \"íšŒìƒ‰\", 9: \"í°ìƒ‰\"\n",
    "    }\n",
    "    multipliers = {\n",
    "        -2: \"ì€ìƒ‰\", -1: \"ê¸ˆìƒ‰\", 0: \"ê²€ì •\", 1: \"ê°ˆìƒ‰\", 2: \"ë¹¨ê°•\", 3: \"ì£¼í™©\",\n",
    "        4: \"ë…¸ë‘\", 5: \"ì´ˆë¡\", 6: \"íŒŒë‘\", 7: \"ë³´ë¼\"\n",
    "    }\n",
    "\n",
    "    s = f\"{res_val:.10f}\"\n",
    "    if '.' in s:\n",
    "        s = s.rstrip('0').rstrip('.')\n",
    "\n",
    "    if float(s) < 10:\n",
    "        first_digit = int(s[0])\n",
    "        second_digit = int(s[2]) if len(s) > 2 else 0\n",
    "        exponent = -1\n",
    "    else:\n",
    "        significant_figs = s.replace('.', '')\n",
    "        first_digit = int(significant_figs[0])\n",
    "        second_digit = int(significant_figs[1])\n",
    "        exponent = len(s.split('.')[0]) - 2\n",
    "\n",
    "    band1 = f\"{colors[first_digit]}({first_digit})\"\n",
    "    band2 = f\"{colors[second_digit]}({second_digit})\"\n",
    "    multiplier_color = multipliers.get(exponent, \"ì•Œ ìˆ˜ ì—†ìŒ\")\n",
    "    multiplier = f\"{multiplier_color}(x10^{exponent})\"\n",
    "\n",
    "    return f\"1ë°´ë“œ: {band1} | 2ë°´ë“œ: {band2} | 3ë°´ë“œ(ìŠ¹ìˆ˜): {multiplier} | 4ë°´ë“œ(ì˜¤ì°¨): ê¸ˆìƒ‰(Â±5%)\"\n",
    "\n",
    "# AI íšŒë¡œ ë¬¸ì œ í’€ì´ íƒ­ì˜ generate_and_solve_problem í•¨ìˆ˜ ë‚´ questions ë”•ì…”ë„ˆë¦¬ë„ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤.\n",
    "# ì˜ˆì‹œ:\n",
    "# questions = {\n",
    "#     'R_total': \"### **ë¬¸ì œ: ìœ„ íšŒë¡œì˜ ì „ì²´ ë“±ê°€ ì €í•­(R_total)ì€ ì–¼ë§ˆì¼ê¹Œìš”?**\",\n",
    "#     'I_total': \"### **ë¬¸ì œ: ìœ„ íšŒë¡œì— íë¥´ëŠ” ì „ì²´ ì „ë¥˜(I_total)ëŠ” ì–¼ë§ˆì¼ê¹Œìš”?**\",\n",
    "#     'V1': \"### **ë¬¸ì œ: ì €í•­ R1ì— ê±¸ë¦¬ëŠ” ì „ì••(V1)ì€ ì–¼ë§ˆì¼ê¹Œìš”?**\",\n",
    "#     'V2': \"### **ë¬¸ì œ: ì €í•­ R2ì— ê±¸ë¦¬ëŠ” ì „ì••(V2)ì€ ì–¼ë§ˆì¼ê¹Œìš”?**\"\n",
    "\n",
    "def summarize_logic(query: str) -> str:\n",
    "    # ì´ í•¨ìˆ˜ëŠ” ì›ë˜ì˜ ë³µì¡í•œ ìš”ì•½ íŒŒì´í”„ë¼ì¸(ë¶„ë¥˜,í‰ê°€,ë¶„í•´,ì¶”ì¶œ,í†µí•©,ìš”ì•½)ì„ ëŒ€í‘œí•©ë‹ˆë‹¤.\n",
    "    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "    return f\"'{query}'ì— ëŒ€í•œ ìš”ì•½ ê²°ê³¼ì…ë‹ˆë‹¤. (ì´ê³³ì— LLMì˜ ìƒì„¸ ìš”ì•½ì´ ìƒì„±ë©ë‹ˆë‹¤.)\"\n",
    "\n",
    "def _summarize_chat_handler(user_text: str, history: list[list[str]]) -> str:\n",
    "    return summarize_logic(user_text)\n",
    "\n",
    "def ohms_law(V: float = None, I: float = None, R: float = None) -> Dict[str, Any]:\n",
    "    try:\n",
    "        known = sum(x is not None for x in [V, I, R])\n",
    "        if known < 2: raise ValueError(\"ì„¸ ë³€ìˆ˜(V, I, R) ì¤‘ ìµœì†Œ 2ê°œê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        if V is None: V = I * R\n",
    "        if I is None:\n",
    "            if R == 0: raise ValueError(\"R=0ì´ë©´ Ië¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            I = V / R\n",
    "        if R is None:\n",
    "            if I == 0: raise ValueError(\"I=0ì´ë©´ Rì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            R = V / I\n",
    "        P = V * I\n",
    "        return {\"V[V]\": V, \"I[A]\": I, \"R[Î©]\": R, \"P[W]\": P}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# ----------------------------->\n",
    "# 3. Gradio UI ë° ì‹ ê·œ ê¸°ëŠ¥ êµ¬í˜„\n",
    "# ----------------------------->\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"ì „ê¸°Â·ì „ì ì¢…í•© ì–´ì‹œìŠ¤í„´íŠ¸\") as demo:\n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "        <div style=\"text-align: center; max-width: 820px; margin: 0 auto; padding: 18px;\">\n",
    "            <h1 style=\"color: #007bff; font-size: 2.1em; font-weight: bold; margin-bottom: 4px;\">ğŸ”Œ ì „ê¸°Â·ì „ì ì¢…í•© ì–´ì‹œìŠ¤í„´íŠ¸</h1>\n",
    "            <p style=\"color: #555; font-size: 1.02em;\">ìš”ì•½, ê³„ì‚°, ëŒ€í™”, ë¬¸ì„œ ë¶„ì„ê¹Œì§€ í•˜ë‚˜ì˜ íˆ´ì—ì„œ í•´ê²°í•˜ì„¸ìš”.</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # íƒ­ 1: ì‹ ê·œ í†µí•© ì±—ë´‡\n",
    "    with gr.Tab(\"ğŸ’¬ ì „ê¸° ì±—ë´‡\"):\n",
    "        gr.Markdown(\"### ì „ê¸°ê³µí•™ ì§ˆë¬¸ì„ ë‹µí•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "        # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "        chat_memory = gr.State(lambda: ConversationBufferMemory(memory_key=\"history\", return_messages=True))\n",
    "\n",
    "        def unified_chat_handler(message, history, memory):\n",
    "            chat_history = memory.load_memory_variables({})['history']\n",
    "\n",
    "            # ëŒ€í™” ê¸°ë¡ì´ ë¹„ì–´ìˆê³ , ì…ë ¥ëœ ë©”ì‹œì§€ê°€ ìš”ì•½ ìš”ì²­ê³¼ ìœ ì‚¬í•  ê²½ìš°\n",
    "            # ì •êµí•œ ìš”ì•½ íŒŒì´í”„ë¼ì¸ì„ ë¨¼ì € ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë¶„ê¸° ì²˜ë¦¬)\n",
    "            # ì—¬ê¸°ì„œëŠ” ëª¨ë“  ì…ë ¥ì„ ëŒ€í™”í˜•ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "\n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"You are a helpful assistant specializing in electrical engineering. You can summarize topics and hold deep conversations.\"),\n",
    "                MessagesPlaceholder(variable_name=\"history\"),\n",
    "                (\"human\", \"{input}\")\n",
    "            ])\n",
    "            chain = prompt | llm_gen\n",
    "\n",
    "            response = chain.invoke({\"input\": message, \"history\": chat_history})\n",
    "\n",
    "            memory.save_context({\"input\": message}, {\"output\": response.content})\n",
    "            history.append((message, response.content))\n",
    "            return \"\", history, memory\n",
    "\n",
    "        chatbot_display = gr.Chatbot(height=520, show_copy_button=True, label=\"ëŒ€í™”ì°½\")\n",
    "        msg_input = gr.Textbox(placeholder=\"ì•„ë˜ ì˜ˆì‹œë¥¼ í´ë¦­í•˜ê±°ë‚˜ ì§ì ‘ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\", label=\"ì§ˆë¬¸ ì…ë ¥\")\n",
    "\n",
    "        gr.Examples(\n",
    "            examples=[\"í•œêµ­ ë„ë§¤ ì „ë ¥ì‹œì¥ ê°€ê²© ê²°ì • êµ¬ì¡° ìš”ì•½\", \"BESS ì‹œì¥ íŠ¸ë Œë“œ í•µì‹¬ í¬ì¸íŠ¸\", \"EV ì¶©ì „ ì¸í”„ë¼ ìµœê·¼ ì´ìŠˆ ì •ë¦¬\"],\n",
    "            inputs=msg_input,\n",
    "            label=\"ì˜ˆì‹œ ì§ˆë¬¸\"\n",
    "        )\n",
    "\n",
    "        clear_btn = gr.Button(\"ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ (ê¸°ë¡ ì‚­ì œ)\")\n",
    "\n",
    "        msg_input.submit(unified_chat_handler, [msg_input, chatbot_display, chat_memory], [msg_input, chatbot_display, chat_memory])\n",
    "        clear_btn.click(lambda: (None, \"\", ConversationBufferMemory(memory_key=\"history\", return_messages=True)), None, [chatbot_display, msg_input, chat_memory], queue=False)\n",
    "\n",
    "\n",
    "    # íƒ­ 3: ì‹ ê·œ - ë¬¸ì„œ ê¸°ë°˜ Q&A\n",
    "    with gr.Tab(\"ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ Q&A\"):\n",
    "        gr.Markdown(\"### PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”.\")\n",
    "        retriever_state = gr.State(None)\n",
    "\n",
    "        def process_document(file):\n",
    "            if file is None: return None, \"íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
    "            try:\n",
    "                loader = PyPDFLoader(file.name)\n",
    "                documents = loader.load()\n",
    "                if not documents: return None, \"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "                splits = text_splitter.split_documents(documents)\n",
    "                vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever, f\"'{os.path.basename(file.name)}' ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "            except Exception as e:\n",
    "                return None, f\"ì˜¤ë¥˜ ë°œìƒ: {e}\"\n",
    "\n",
    "        def document_qa_handler(message, history, retriever):\n",
    "            if retriever is None:\n",
    "                history.append((message, \"ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\"))\n",
    "                return \"\", history\n",
    "\n",
    "            contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"Given a chat history and the latest user question, formulate a standalone question.\"),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ])\n",
    "            history_aware_retriever = create_history_aware_retriever(llm_gen, retriever, contextualize_q_prompt)\n",
    "            qa_system_prompt = \"Answer the user's question based on the below context:\\n\\n{context}\"\n",
    "            qa_prompt = ChatPromptTemplate.from_messages([(\"system\", qa_system_prompt), (\"human\", \"{input}\")])\n",
    "            question_answer_chain = create_stuff_documents_chain(llm_gen, qa_prompt)\n",
    "            rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "            chat_history_messages = []\n",
    "            for human, ai in history:\n",
    "                chat_history_messages.append(HumanMessage(content=human))\n",
    "                chat_history_messages.append(AIMessage(content=ai))\n",
    "\n",
    "            response = rag_chain.invoke({\"input\": message, \"chat_history\": chat_history_messages})\n",
    "            history.append((message, response[\"answer\"]))\n",
    "            return \"\", history\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                file_uploader = gr.File(label=\"PDF íŒŒì¼ ì—…ë¡œë“œ\")\n",
    "                upload_status = gr.Textbox(label=\"ì—…ë¡œë“œ ìƒíƒœ\", interactive=False)\n",
    "            with gr.Column(scale=2):\n",
    "                doc_chatbot = gr.Chatbot(label=\"ë¬¸ì„œ Q&A\", height=450)\n",
    "                doc_textbox = gr.Textbox(label=\"ì§ˆë¬¸ ì…ë ¥\", placeholder=\"ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...\")\n",
    "\n",
    "        file_uploader.upload(fn=process_document, inputs=[file_uploader], outputs=[retriever_state, upload_status], show_progress=\"full\")\n",
    "        doc_textbox.submit(fn=document_qa_handler, inputs=[doc_textbox, doc_chatbot, retriever_state], outputs=[doc_textbox, doc_chatbot])\n",
    "\n",
    "    # íƒ­ 4: ê³µí•™ ê³„ì‚°ê¸°\n",
    "    with gr.Tab(\"ğŸ§® ê³µí•™ ê³„ì‚°ê¸°\"):\n",
    "        gr.Markdown(\"#### 1) ì˜´ì˜ ë²•ì¹™ (V=IR, P=VI)\")\n",
    "        with gr.Row():\n",
    "            V_in = gr.Number(label=\"V [Volt]\", value=None)\n",
    "            I_in = gr.Number(label=\"I [Ampere]\", value=None)\n",
    "            R_in = gr.Number(label=\"R [Ohm]\", value=None)\n",
    "        ohm_btn = gr.Button(\"ê³„ì‚°\")\n",
    "        ohm_out = gr.JSON(label=\"ê²°ê³¼\")\n",
    "        def ohm_cb(V, I, R):\n",
    "            try:\n",
    "                return ohms_law(V, I, R)\n",
    "            except Exception as e:\n",
    "                return {\"error\": str(e)}\n",
    "        ohm_btn.click(ohm_cb, [V_in, I_in, R_in], ohm_out)\n",
    "\n",
    "        gr.Markdown(\"---\")\n",
    "        gr.Markdown(\"#### 2) ì„í”¼ë˜ìŠ¤ ê³„ì‚° (RLC)\")\n",
    "        with gr.Row():\n",
    "            R_rlc = gr.Number(label=\"R [Î©] (ì§ë ¬/ë³‘ë ¬ ê³µí†µ)\", value=0.0)\n",
    "            L_rlc = gr.Number(label=\"L [H]\", value=0.0)\n",
    "            C_rlc = gr.Number(label=\"C [F]\", value=0.0)\n",
    "            f_rlc = gr.Number(label=\"ì£¼íŒŒìˆ˜ f [Hz]\", value=60.0)\n",
    "        mode = gr.Radio(choices=[\"ì§ë ¬\", \"ë³‘ë ¬\"], value=\"ì§ë ¬\", label=\"ê²°ì„  ë°©ì‹\")\n",
    "        rlc_btn = gr.Button(\"ì„í”¼ë˜ìŠ¤ ê³„ì‚°\")\n",
    "        rlc_out = gr.JSON(label=\"Z ê²°ê³¼\")\n",
    "        def rlc_cb(R, L, C, f, m):\n",
    "            try:\n",
    "                if m == \"ì§ë ¬\":\n",
    "                    return impedance_rlc_series(R or 0.0, L or 0.0, C or 0.0, f or 60.0)\n",
    "                else:\n",
    "                    # ë³‘ë ¬ì€ None í—ˆìš© (ì—†ëŠ” ì†Œì)\n",
    "                    Rv = None if (R is None or R == 0) else R\n",
    "                    Lv = None if (L is None or L == 0) else L\n",
    "                    Cv = None if (C is None or C == 0) else C\n",
    "                    return impedance_rlc_parallel(Rv, Lv, Cv, f or 60.0)\n",
    "            except Exception as e:\n",
    "                return {\"error\": str(e)}\n",
    "        rlc_btn.click(rlc_cb, [R_rlc, L_rlc, C_rlc, f_rlc, mode], rlc_out)\n",
    "\n",
    "        gr.Markdown(\"---\")\n",
    "        gr.Markdown(\"#### 3) ë²¡í„° ë¯¸ì ë¶„ (grad / div / curl)\")\n",
    "        op_vec = gr.Radio(choices=[\"grad\",\"div\",\"curl\"], value=\"grad\", label=\"ì—°ì‚°ì\")\n",
    "        expr_vec = gr.Textbox(\n",
    "            label=\"í‘œí˜„ì‹: gradëŠ” ìŠ¤ì¹¼ë¼ 1ê°œ / div, curlì€ [Fx, Fy, Fz] ì¤„ë°”ê¿ˆ 3ê°œ\",\n",
    "            value=\"x**2*y\"\n",
    "        )\n",
    "        vars_vec = gr.Textbox(label=\"ë³€ìˆ˜ ìˆœì„œ(ê³µë°± êµ¬ë¶„) ì˜ˆ: x y z\", value=\"x y z\")\n",
    "        vec_btn = gr.Button(\"ê³„ì‚°\")\n",
    "        vec_out = gr.JSON(label=\"ê²°ê³¼\")\n",
    "        def vec_cb(op, expr_block, var_order):\n",
    "            try:\n",
    "                exprs = [s.strip() for s in expr_block.splitlines() if s.strip()]\n",
    "                return vector_calculus(op, exprs, var_order)\n",
    "            except Exception as e:\n",
    "                return {\"error\": str(e)}\n",
    "        vec_btn.click(vec_cb, [op_vec, expr_vec, vars_vec], vec_out)\n",
    "\n",
    "        gr.Markdown(\"---\")\n",
    "        gr.Markdown(\"#### 4) ë¯¸ë¶„/ì •ì ë¶„\")\n",
    "        op_cal = gr.Radio(choices=[\"diff\",\"int\"], value=\"diff\", label=\"ì—°ì‚°ì\")\n",
    "        expr_cal = gr.Textbox(label=\"í‘œí˜„ì‹ f(x). ì˜ˆ: x**3 + 2*x\", value=\"x**3 + 2*x\")\n",
    "        var_cal = gr.Textbox(label=\"ë³€ìˆ˜ ì´ë¦„\", value=\"x\")\n",
    "        with gr.Row():\n",
    "            a_cal = gr.Textbox(label=\"ì ë¶„ í•˜í•œ a (ì„ íƒ)\", value=\"\")\n",
    "            b_cal = gr.Textbox(label=\"ì ë¶„ ìƒí•œ b (ì„ íƒ)\", value=\"\")\n",
    "        cal_btn = gr.Button(\"ê³„ì‚°\")\n",
    "        cal_out = gr.Textbox(label=\"ê²°ê³¼\")\n",
    "        def cal_cb(op, expr, var, a, b):\n",
    "            try:\n",
    "                a_ = a if a.strip() else None\n",
    "                b_ = b if b.strip() else None\n",
    "                return calculus(op, expr, var, a_, b_)\n",
    "            except Exception as e:\n",
    "                return f\"error: {e}\"\n",
    "        cal_btn.click(cal_cb, [op_cal, expr_cal, var_cal, a_cal, b_cal], cal_out)\n",
    "\n",
    "        gr.Markdown(\"---\")\n",
    "        gr.Markdown(\"#### 5) ì €í•­ ìƒ‰ê¹” ë  ë³€í™˜\")\n",
    "        res_in = gr.Number(label=\"ì €í•­ê°’ [Î©]\", value=47000)\n",
    "        res_btn = gr.Button(\"ìƒ‰ê¹” ë  ë³€í™˜\")\n",
    "        res_out = gr.Textbox(label=\"ê²°ê³¼\")\n",
    "        def res_cb(res_val):\n",
    "            try:\n",
    "                return resistor_color_code(res_val)\n",
    "            except Exception as e:\n",
    "                return f\"ì˜¤ë¥˜: {e}\"\n",
    "        res_btn.click(res_cb, [res_in], res_out)\n",
    "\n",
    "    with gr.Tab(\"ğŸ“š ë‹¨ìœ„ ë° ê¸°í˜¸\"):\n",
    "        gr.Markdown(\"#### ğŸ”Œ ì „ê¸°ê³µí•™ ê¸°ë³¸ ë‹¨ìœ„ ë° ê¸°í˜¸ ì •ë¦¬\")\n",
    "        gr.Markdown(\n",
    "                  \"\"\"\n",
    "      | êµ¬ë¶„ | í•­ëª© | ë‹¨ìœ„ | ê¸°í˜¸ |\n",
    "      |:---|:---|:---|:---|\n",
    "      | **ê¸°ë³¸ëŸ‰** | ì „ë¥˜ (Current) | ì•”í˜ì–´ | A |\n",
    "      | | ì „ì•• (Voltage) | ë³¼íŠ¸ | V |\n",
    "      | | ì €í•­ (Resistance) | ì˜´ | Î© |\n",
    "      | | ì „ë ¥ (Power) | ì™€íŠ¸ | W |\n",
    "      | | ì—ë„ˆì§€ (Energy) | ì¤„, ì™€íŠ¸ì‹œ | J, Wh |\n",
    "      | **êµë¥˜ (AC)** | ì£¼íŒŒìˆ˜ (Frequency) | í—¤ë¥´ì¸  | Hz |\n",
    "      | | ì„í”¼ë˜ìŠ¤ (Impedance) | ì˜´ | Z |\n",
    "      | | ë¦¬ì•¡í„´ìŠ¤ (Reactance) | ì˜´ | X |\n",
    "      | | ì—­ë¥  (Power Factor) | - | pf |\n",
    "      | **ìê¸°ì¥** | ìì† (Magnetic Flux) | ì›¨ë²„ | Wb |\n",
    "      | | ìì† ë°€ë„ (Flux Density) | í…ŒìŠ¬ë¼ | T |\n",
    "      | | ì¸ë•í„´ìŠ¤ (Inductance) | í—¨ë¦¬ | H |\n",
    "      | **ê¸°íƒ€** | íš¨ìœ¨ (Efficiency) | - | Î· |\n",
    "      | | ê³ ì¡°íŒŒ (Harmonics) | - | THD |\n",
    "      | | í¼ìœ ë‹› (Per Unit) | - | pu |\n",
    "                  \"\"\"\n",
    "              )\n",
    "\n",
    "\n",
    "    demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1U1dbtIj4GcmVVCtdG6qRqtV5gutZqipS",
     "timestamp": 1756297245118
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
